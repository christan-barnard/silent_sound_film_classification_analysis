# -*- coding: utf-8 -*-
"""Final ANN Tuned models comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OyWZzUbnFaQiN-C95cjPKmjh1eMBATWM

# Data Preprocessing
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf

# pip install -q -U keras-tuner

"""### Importing the dataset"""

styleSilentShot = pd.read_excel('1.2 SilentSoundScale_asl_count30+.xlsx')
X = styleSilentShot.iloc[: , 1:-1].values # These are the predictor, independent variables (the 1:-1 excludes the first and last columns)
y = styleSilentShot.iloc[: , -1].values # These are the target variables (or classes, in this case)
styleSilentShot.head()

print(X)

print(y)

"""### Encoding categorical data

One Hot Encoding the "COUNTRY" column
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

print(X)

"""### Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train) # Feature scale even the one-hot-encoded variables for deep learning
X_test = sc.transform(X_test) # Not fitted to the test set to avoid information leakage
                              # Using the same scaler (fit mean and std dev) as the training data because 
                              # the test data is "unavailable"

print(X_test)

"""# The Random Search ANN

## Initialise the RS ANN
"""

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard
# %load_ext tensorboard
import datetime, os

# Training parameters
MAX_EPOCHS = 100
BATCH_SIZE = 32

# Early Stopping
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

# Performance metrics dictionary
performance_metrics = {
    'best_val_epoch': [],
    'training_loss': [],
    'training_auc': [],
    'validation_loss': [],
    'validation_auc': [],
    'evaluation_loss': [],
    'evaluation_auc': []
}

# Random search performance dataframe
RS_performance_df=pd.DataFrame(performance_metrics)

# START Training for loop

for i in range(1, 31):

    # Initialise the Ransom Search model:
    rs_model = tf.keras.models.Sequential()
    # Add the input and hidden layers
    rs_model.add(tf.keras.layers.Dense(units=22, kernel_regularizer='l2', activation='relu'))
    # Add the output layer
    rs_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
    # Specify the learning rate based on the tuner results
    rs_learning_rate=0.01
    rs_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=rs_learning_rate),
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['AUC'])

    # Define new TensorBoard directory to keep track of each run
    logdir = os.path.join("training_logs_random_search", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

    # Fit the model
    history = rs_model.fit(X_train, y_train,
              batch_size=BATCH_SIZE,
              epochs=MAX_EPOCHS, 
              validation_split=0.3,
              callbacks=[stop_early, tensorboard_callback])

    # list all data in history
    # print(history.history.keys())
    # print(history.history['loss'][-1], history.history['auc'][-1], history.history['val_loss'][-1], history.history['val_auc'][-1])

    # store the best_epoch value
    val_auc_per_epoch = history.history['val_auc']
    best_epoch = val_auc_per_epoch.index(max(val_auc_per_epoch)) + 1
    print('Best epoch: %d' % (best_epoch,))

    # Evaluate the model
    eval_result = rs_model.evaluate(X_test, y_test)
    print("[test loss, test auc]:", eval_result)
    # print(eval_result[0], eval_result[1])

    # store the training, validation and test data in the RS_performance_df
    trial_performance_row = [
                            best_epoch,
                            history.history['loss'][-1],
                            history.history['auc'][-1],
                            history.history['val_loss'][-1],
                            history.history['val_auc'][-1],
                            eval_result[0],
                            eval_result[1]
                            ]
    # Add the performance row to the dataframe
    RS_performance_df.loc['run_' + str(i)] = trial_performance_row

# Outside the loop

"""## Inspect the model performance"""

RS_performance_df

# Commented out IPython magic to ensure Python compatibility.
#Inspect the training tensorboard
# %tensorboard --logdir training_logs_random_search

"""## Document results

Save the Random Search performance dataframe to an excel file
"""

RS_performance_df.to_excel('RS_30_trials_perfomance_results.xlsx')

"""Create evaluation performance box plot"""

import seaborn as sns
sns.set_theme(style="whitegrid")

ax = sns.boxplot(x=RS_performance_df["evaluation_auc"])
ax.set(xlim=(0.875, 0.925))

figure = ax.get_figure()    
figure.savefig('RS_boxplot.png', dpi=500)

"""# The Bayesian Optimisation ANN

## Initialise the BO ANN
"""

# Bayesian optimisation performance dataframe
BO_performance_df=pd.DataFrame(performance_metrics)

# START Training for loop

for i in range(1, 31):

    # Initialise the BO model:
    bo_model = tf.keras.models.Sequential()
    # Add the input and hidden layers
    bo_model.add(tf.keras.layers.Dense(units=28, kernel_regularizer='l2', activation='relu'))
    # Add the output layer
    bo_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
    # Specify the learning rate based on the tuner results
    bo_learning_rate=0.01
    bo_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=bo_learning_rate),
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['AUC'])

    # Define new TensorBoard directory to keep track of each run
    logdir = os.path.join("training_logs_bayesian_optimisation", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

    # Fit the model
    history = bo_model.fit(X_train, y_train,
              batch_size=BATCH_SIZE,
              epochs=MAX_EPOCHS, 
              validation_split=0.3,
              callbacks=[stop_early, tensorboard_callback])

    # list all data in history
    # print(history.history.keys())
    # print(history.history['loss'][-1], history.history['auc'][-1], history.history['val_loss'][-1], history.history['val_auc'][-1])

    # store the best_epoch value
    val_auc_per_epoch = history.history['val_auc']
    best_epoch = val_auc_per_epoch.index(max(val_auc_per_epoch)) + 1
    print('Best epoch: %d' % (best_epoch,))

    # Evaluate the model
    eval_result = bo_model.evaluate(X_test, y_test)
    print("[test loss, test auc]:", eval_result)
    # print(eval_result[0], eval_result[1])

    # store the training, validation and test data in the BO_performance_df
    trial_performance_row = [
                            best_epoch,
                            history.history['loss'][-1],
                            history.history['auc'][-1],
                            history.history['val_loss'][-1],
                            history.history['val_auc'][-1],
                            eval_result[0],
                            eval_result[1]
                            ]
    # Add the performance row to the dataframe
    BO_performance_df.loc['run_' + str(i)] = trial_performance_row

# Outside the loop:

"""## Inspect the model performance"""

# Display the BO performance dataframe
BO_performance_df

# Commented out IPython magic to ensure Python compatibility.
#Inspect the training tensorboard
# %tensorboard --logdir training_logs_bayesian_optimisation

"""## Document results

Save the Random Search performance dataframe to an excel file
"""

BO_performance_df.to_excel('BO_30_trials_perfomance_results.xlsx')

"""Create evaluation performance box plot"""

ax = sns.boxplot(x=BO_performance_df["evaluation_auc"])
ax.set(xlim=(0.875, 0.925))

figure = ax.get_figure()    
figure.savefig('BO_boxplot.png', dpi=500)

"""# Model comparison

In this section, the models using on the Adam optimiser are compared to the models using on the L-BFGS-B optimiser. The L-BFGS-B models were constructed in the **ANN 5x2 cv t-test** python script for comparative purposes and also in Orange (which is also based on the scikit learn python implementation of the model) to obtain training and testing results.

These results were tabulated in the **L-BFGS-B vs Adam boxplot data test auc.xlsx** document, which is also contained in this GitHub repository.

## Boxplots comparison
"""

optimiserComparisonBoxplots = pd.read_excel('L-BFGS-B vs Adam boxplot data test auc.xlsx')
optimiserComparisonBoxplots.head()

import seaborn as sns
sns.set_theme(style="whitegrid")

plt.rcParams["font.family"] = "serif"
plt.rcParams.update({'font.size': 15})
fig_dims = (9, 4)
fig, ax = plt.subplots(figsize = fig_dims)

ax = sns.boxplot(x="Neurons in hidden layer", y="Test AUC", hue="Optimiser",
                 data=optimiserComparisonBoxplots, palette="Blues")
ax.set(ylim=(0.892, 0.98))

figure = ax.get_figure()    
figure.savefig('FourModelsComparisonBoxplots.png', dpi=500)

"""Compare only the two models that have 22 neurons in the hidden layer"""

# Construct a datframe containing only the random search 22-neuron models data
randomSearchComparisonBoxplots = optimiserComparisonBoxplots[optimiserComparisonBoxplots['Neurons in hidden layer'] != 28]
randomSearchComparisonBoxplots.head()

plt.rcParams["font.family"] = "serif"
plt.rcParams.update({'font.size': 15})
fig_dims = (9, 5)
fig, ax = plt.subplots(figsize = fig_dims)

ax = sns.boxplot(x="Optimiser", y="Test AUC",
                 data=randomSearchComparisonBoxplots, palette="Blues")
ax.set(ylim=(0.892, 0.98))

figure = ax.get_figure()    
figure.savefig('RandomSearchComparisonBoxplots.png', dpi=500)

# Construct a datframe containing only the bayesian optimisation 28-neuron models data
BayesComparisonBoxplots = optimiserComparisonBoxplots[optimiserComparisonBoxplots['Neurons in hidden layer'] != 22]
BayesComparisonBoxplots.head()

plt.rcParams["font.family"] = "serif"
plt.rcParams.update({'font.size': 15})
fig_dims = (9, 5)
fig, ax = plt.subplots(figsize = fig_dims)

ax = sns.boxplot(x="Optimiser", y="Test AUC",
                 data=BayesComparisonBoxplots, palette="Blues")
ax.set(ylim=(0.892, 0.98))

figure = ax.get_figure()    
figure.savefig('BayesComparisonBoxplots.png', dpi=500)

# Construct a datframe containing only the bayesian optimisation 28-neuron models data
AdamComparisonBoxplots = optimiserComparisonBoxplots[optimiserComparisonBoxplots['Optimiser'] != 'L-BFGS-B']
AdamComparisonBoxplots.head()

plt.rcParams["font.family"] = "serif"
plt.rcParams.update({'font.size': 15})
fig_dims = (9, 5)
fig, ax = plt.subplots(figsize = fig_dims)

ax = sns.boxplot(x="Neurons in hidden layer", y="Test AUC",
                 data=AdamComparisonBoxplots, palette="Blues")
ax.set(ylim=(0.892, 0.93))

figure = ax.get_figure()    
figure.savefig('AdamComparisonBoxplots.png', dpi=500)