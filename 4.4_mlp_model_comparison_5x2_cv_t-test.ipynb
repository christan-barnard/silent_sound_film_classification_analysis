{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final ANN 5x2 cv t-test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6JLo1tGNBg"
      },
      "source": [
        "# ANN 5x2 cv t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZyYmS_UE_L"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxkJoQBkUIHC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0Q3aoKUCRX"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKWAkFVGUU0Z"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "MXUkhkMfU4wq",
        "outputId": "82920d4e-1d56-4e48-8b61-b7bdc5aca0c7"
      },
      "source": [
        "styleSilentShot = pd.read_excel('1.2 SilentSoundScale_asl_count30+.xlsx')\n",
        "X = styleSilentShot.iloc[: , 1:-1].values # These are the predictor, independent variables (the 1:-1 excludes the first and last columns)\n",
        "y = styleSilentShot.iloc[: , -1].values # These are the target variables (or classes, in this case)\n",
        "styleSilentShot.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>COUNTRY</th>\n",
              "      <th>ASL</th>\n",
              "      <th>RA</th>\n",
              "      <th>POV</th>\n",
              "      <th>INS</th>\n",
              "      <th>BCU</th>\n",
              "      <th>CU</th>\n",
              "      <th>MCU</th>\n",
              "      <th>MS</th>\n",
              "      <th>MLS</th>\n",
              "      <th>LS</th>\n",
              "      <th>VLS</th>\n",
              "      <th>Sound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10 Things I Hate About You</td>\n",
              "      <td>USA</td>\n",
              "      <td>6.7</td>\n",
              "      <td>58</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>64</td>\n",
              "      <td>224</td>\n",
              "      <td>82</td>\n",
              "      <td>37</td>\n",
              "      <td>36</td>\n",
              "      <td>53</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adventures of Robin Hood, The</td>\n",
              "      <td>USA</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>53</td>\n",
              "      <td>71</td>\n",
              "      <td>77</td>\n",
              "      <td>109</td>\n",
              "      <td>111</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Affairs of Anatole, The</td>\n",
              "      <td>USA</td>\n",
              "      <td>8.0</td>\n",
              "      <td>23</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>33</td>\n",
              "      <td>26</td>\n",
              "      <td>89</td>\n",
              "      <td>141</td>\n",
              "      <td>136</td>\n",
              "      <td>70</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alley Cat</td>\n",
              "      <td>BRI</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17</td>\n",
              "      <td>68</td>\n",
              "      <td>84</td>\n",
              "      <td>82</td>\n",
              "      <td>101</td>\n",
              "      <td>142</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Almost Perfect Affair, An</td>\n",
              "      <td>USA</td>\n",
              "      <td>4.2</td>\n",
              "      <td>64</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>70</td>\n",
              "      <td>199</td>\n",
              "      <td>93</td>\n",
              "      <td>51</td>\n",
              "      <td>26</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Title COUNTRY  ASL  RA  ...  MLS   LS  VLS  Sound\n",
              "0     10 Things I Hate About You     USA  6.7  58  ...   36   53    3      1\n",
              "1  Adventures of Robin Hood, The     USA  5.0   0  ...  109  111   76      1\n",
              "2        Affairs of Anatole, The     USA  8.0  23  ...  136   70    5      0\n",
              "3                      Alley Cat     BRI  6.0  10  ...  101  142    5      0\n",
              "4      Almost Perfect Affair, An     USA  4.2  64  ...   26   51    9      1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYP9cQTWbzuI"
      },
      "source": [
        "# print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38vKGE6Nb2RR"
      },
      "source": [
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6bQ0UgSU-NJ"
      },
      "source": [
        "### Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUxGZezpbMcb"
      },
      "source": [
        "One Hot Encoding the \"COUNTRY\" column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMXC8-KMVirw"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcxwEon-b8nV",
        "outputId": "9980b336-c3e7-4bd3-8d3a-6a0db24b05e0"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 0.0 ... 36 53 3]\n",
            " [0.0 0.0 0.0 ... 109 111 76]\n",
            " [0.0 0.0 0.0 ... 136 70 5]\n",
            " ...\n",
            " [0.0 0.0 1.0 ... 99 66 40]\n",
            " [0.0 0.0 1.0 ... 123 243 26]\n",
            " [0.0 0.0 1.0 ... 83 396 21]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHol938cW8zd"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-TDt0Y_XEfc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "# random_state: Pass an int for reproducible output across multiple function calls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE_FcHyfV3TQ"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViCrE00rV8Sk"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train) # Feature scale even the one-hot-encoded variables for deep learning\n",
        "X_test = sc.transform(X_test) # Not fitted to the test set to avoid information leakage\n",
        "                              # Using the same scaler (fit mean and std dev) as the training data because \n",
        "                              # the test data is \"unavailable\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnA5Ofrw83FN"
      },
      "source": [
        "# print(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKicJzz12Bi-"
      },
      "source": [
        "# Feature scaling for hypothesis testing\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqxeGdUV1A8O"
      },
      "source": [
        "# Construct Adam Scikit Learn Neural Network models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyZuliUV1G-V",
        "outputId": "c904daac-e7f3-41c1-ee4b-cd6138d70f26"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf_adam_1 = MLPClassifier(solver='adam', alpha=0.01,\n",
        "                     hidden_layer_sizes=(22), max_iter=31)\n",
        "clf_adam_2 = MLPClassifier(solver='adam', alpha=0.01,\n",
        "                     hidden_layer_sizes=(28), max_iter=31)\n",
        "\n",
        "score1 = clf_adam_1.fit(X_train, y_train).score(X_test, y_test)\n",
        "score2 = clf_adam_2.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "print('22-neuron Adam model prediction accuracy: %.2f%%' % (score1*100))\n",
        "print('28-neuron Adam model prediction accuracy: %.2f%%' % (score2*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22-neuron Adam model prediction accuracy: 83.19%\n",
            "28-neuron Adam model prediction accuracy: 73.45%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnSLCX1S1TYm"
      },
      "source": [
        "# Implement the 5x2 cv paired t-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nei_9GrPzGqF"
      },
      "source": [
        "H0: Both models have the same performance on the dataset.\n",
        "\n",
        "H1: Both models doesn’t have the same performance on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHwkYf2izHiV"
      },
      "source": [
        "Paired-samples t-test:\n",
        "\n",
        "1. If the result of the test suggests that there is insufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to statistical chance.\n",
        "2. If the result of the test suggests that there is sufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to a difference in the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv5SDHbB1dat"
      },
      "source": [
        "from mlxtend.evaluate import paired_ttest_5x2cv\n",
        "\n",
        "t, p = paired_ttest_5x2cv(estimator1=clf_adam_1,\n",
        "                          estimator2=clf_adam_2,\n",
        "                          X=X, y=y,\n",
        "                          random_seed=1)\n",
        "\n",
        "# If p > α (0.05), we cannot reject the null hypothesis and may conclude that the performance of the two algorithms is not significantly different\n",
        "print('t statistic: %.3f' % t)\n",
        "print('p value: %.3f' % p)\n",
        "\n",
        "# interpret the result\n",
        "if p <= 0.05:\n",
        "\tprint('At a confidence of level of 0.95, the result of the test suggests that there is sufficient evidence to reject the null hypothesis. The difference between mean test auc performance is likely due to a difference in the models.')\n",
        "else:\n",
        "\tprint('At a confidence of level of 0.95, the result of the test suggests that there is insufficient evidence to reject the null hypothesis. The difference between mean test auc performance is likely due to statistical chance.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HeAMvXIv3m8"
      },
      "source": [
        "# Construct L-BFGS Scikit Learn Neural Network models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nU5wO8Iwlyd",
        "outputId": "9c7c6b0f-5483-4478-e685-7d1adec64f01"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf_lbfgs_1 = MLPClassifier(solver='lbfgs', alpha=0.01,\n",
        "                     hidden_layer_sizes=(22), max_iter=12)\n",
        "clf_lbfgs_2 = MLPClassifier(solver='lbfgs', alpha=0.01,\n",
        "                     hidden_layer_sizes=(28), max_iter=12)\n",
        "\n",
        "score1 = clf_lbfgs_1.fit(X_train, y_train).score(X_test, y_test)\n",
        "score2 = clf_lbfgs_2.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "print('22-neuron model prediction accuracy: %.2f%%' % (score1*100))\n",
        "print('28-neuron model prediction accuracy: %.2f%%' % (score2*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22-neuron model prediction accuracy: 82.30%\n",
            "28-neuron model prediction accuracy: 84.96%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GHPHpjMywVH"
      },
      "source": [
        "# Implement the 5x2 cv paired t-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNi8oE_Syvse"
      },
      "source": [
        "from mlxtend.evaluate import paired_ttest_5x2cv\n",
        "\n",
        "t, p = paired_ttest_5x2cv(estimator1=clf_lbfgs_1,\n",
        "                          estimator2=clf_lbfgs_2,\n",
        "                          X=X, y=y,\n",
        "                          random_seed=1)\n",
        "\n",
        "# If p > α (0.05), we cannot reject the null hypothesis and may conclude that the performance of the two algorithms is not significantly different\n",
        "print('t statistic: %.3f' % t)\n",
        "print('p value: %.3f' % p)\n",
        "\n",
        "# interpret the result\n",
        "if p <= 0.05:\n",
        "\tprint('At a confidence of level of 0.95, the result of the test suggests that there is sufficient evidence to reject the null hypothesis. The difference between mean test auc performance is likely due to a difference in the models.')\n",
        "else:\n",
        "\tprint('At a confidence of level of 0.95, the result of the test suggests that there is insufficient evidence to reject the null hypothesis. The difference between mean test auc performance is likely due to statistical chance.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}