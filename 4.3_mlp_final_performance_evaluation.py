# -*- coding: utf-8 -*-
"""Final Keras Tuner ANN Tuner Silent-Sound Shot Scale Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_BPLdx74xEiHHhkYBrnWtg7vNMla8yRe

# Artificial Neural Network

### Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow import keras

pip install -q -U keras-tuner

import kerastuner as kt

tf.__version__

"""# Data Preprocessing

### Importing the dataset
"""

styleSilentShot = pd.read_excel('1.2 SilentSoundScale_asl_count30+.xlsx')
X = styleSilentShot.iloc[: , 1:-1].values # These are the predictor, independent variables (the 1:-1 excludes the first and last columns)
y = styleSilentShot.iloc[: , -1].values # These are the target variables (or classes, in this case)
styleSilentShot.head()

# print(X)

# print(y)

"""### Encoding categorical data

One Hot Encoding the "COUNTRY" column
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(X))

print(X)

"""### Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)
# random_state: Pass an int for reproducible output across multiple function calls

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train) # Feature scale even the one-hot-encoded variables for deep learning
X_test = sc.transform(X_test) # Not fitted to the test set to avoid information leakage
                              # Using the same scaler (fit mean and std dev) as the training data because 
                              # the test data is "unavailable"

# print(X_test)

"""# Keras Tuner ANN model with layer hypertuning

Adapted from: https://keras-team.github.io/keras-tuner/

## Define the model builder function
"""

tf.keras.backend.clear_session()

def build_model(hp):
    model = keras.Sequential()

    # Dropout function: keras.layers.Dropout(0.3)

    # Hyperparameter specifying the number of hidden layers
    for i in range(hp.Int('num_layers', 1, 5)):
        model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=2,
                                            max_value=28,
                                            step=2),
                                        kernel_regularizer='l2', # the default values used are l1=0.01 and l2=0.01
                                        activation='relu'))
    
    # Define the output layer with units=1 for a binary output
    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # activation = 'softmax' for non-binary

    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                loss=keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=['AUC'])
    
    # I use AUC here, but val_auc for the tuner, splitting the training data for validation. 
    # When training the final model, I do not want to split the data further
    # (i.e. validation_split=0.3) ---  I want to train the model with the best hyperparameters identified during
    # tuning on the FULL training data set and thereafter evaluate it on the test data.

    return model

"""## Instantiate the tuner"""

from kerastuner.tuners import BayesianOptimization
from kerastuner.tuners import RandomSearch

# 50 Bayesian combinations with 30 trials per combination
MAX_TRIALS = 50
EXECUTION_PER_TRIAL = 30
MAX_EPOCHS = 100
BATCH_SIZE = 32

tuner = BayesianOptimization (
    build_model,
    objective=kt.Objective('val_auc', direction='max'),
    max_trials=MAX_TRIALS,
    executions_per_trial=EXECUTION_PER_TRIAL,
    directory='my_dir',
    project_name='silent_sound_ann_bayesian_search_1.1')

# tuner = RandomSearch(
#     build_model,
#     objective=kt.Objective('val_auc', direction='max'),
#     max_trials=MAX_TRIALS,
#     executions_per_trial=EXECUTION_PER_TRIAL,
#     directory='my_dir',
#     project_name='silent_sound_ann_random_search_1.1')

# Note from Keras: the purpose of having multiple executions per trial is to reduce
# results variance and therefore be able to more accurately assess the performance of a model.

"""Print a summary of the search space"""

tuner.search_space_summary()

"""Implement an early stopping mechanism"""

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

# This callback will stop the training when there is no improvement in
# the validation loss for five consecutive epochs.

"""## Search for the best hyperparameter configuration"""

# Run the Search tuner for MAX_TRIALS combinations, EXECUTION_PER_TRIAL executions per combinationa and 30 epochs per trial.

tuner.search(X_train, y_train,
             batch_size = BATCH_SIZE,
             epochs=MAX_EPOCHS, # The best_epochs value in subsequent sections is usually between 30 and 100
             validation_split=0.3,
             callbacks=[stop_early]) 

             # The validation performance is the criterion by which the best hyperparameter configuration is chosen

"""Summary of the results"""

tuner.results_summary(num_trials=10)

"""## Retrain the model with the best hyperparameters

Save the best hyperparameters
"""

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0] # num_trials=2 will return the hp's of the best 2 trials

"""Create a log directory for Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import datetime, os

logdir = os.path.join("history_logs_random_search", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

"""Over-fitting: Retrain the model and identify the best epoch (epoch at which the largest val_auc is achieved)"""

# Build the model with the optimal hyperparameters and train it on the data for 100 epochs
model = tuner.hypermodel.build(best_hps)
history = model.fit(X_train, y_train,
                    batch_size = BATCH_SIZE,
                    epochs = 100,
                    validation_split=0.3,
                    callbacks=[tensorboard_callback]) 

# Overfitting: After about 20 epochs, the model fits the data with an AUC of 1.00, but the val_AUC declines from about 0.92 to 0.82
# Determine the optimal number of epochs based on the validation data

# Validation Split (validation_split=0.3)
# Fraction of the training data to be used as validation data.
# The model will set apart this fraction of the training data, will not train on it, and will evaluate
# the loss and any model metrics on this data at the end of each epoch.
# The validation data is selected from the last samples in the x and y data provided, before shuffling. 

val_auc_per_epoch = history.history['val_auc']
best_epoch = val_auc_per_epoch.index(max(val_auc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

"""## Inspect the training Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir history_logs_random_search

"""## Train the final hypermodel

This section exists so as to evaluate the model performance on the test set.

Re-instantiate the hypermodel and train it with the optimal number of epochs from above.
"""

hypermodel = tuner.hypermodel.build(best_hps)

# Retrain the model
hypermodel.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs=best_epoch) # best_epoch

"""## Evaluate the hypermodel on the test data"""

eval_result = hypermodel.evaluate(X_test, y_test)
print("[test loss, test auc]:", eval_result)